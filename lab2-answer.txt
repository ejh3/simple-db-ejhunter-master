I initially had a nested loop join for all cases. The first query took under a second but the second and third never finished. The second produced some tuples but the third didn't produce any, though I didn't let it run more than a few minutes.

Then I implemented a single pass hashjoin for equality joins and my query times became 0.59, 1.53 and 2.48 seconds on attu and all under one second on my laptop with i-8650U.

In this lab we finished some more of the most basic components of a database system. Basic operators like filters, joins and aggregates as well as the ability to modify tables. Also a page eviction policy which is necessary if you ever want to work with databases larger than what fits in memory.

I found the way Aggregate was implemented to be pretty interesting. Rather than there being different operators for joining strings or integers, there is only one operator which can do both. The implementation for aggregation of strings or integers is kept to classes which inherit from Aggregator, which Aggregate chooses between based on the types of the field it's aggregating. Doing it this way factors out some code that would otherwise be duplicated between the operators.

There is a long chain of calls to different classes insertTuple (or deleteTuple) once the top level is called from the bufferpool. It's best to alter the table through the bufferpool so it can keep track of what pages have been accessed most recently for the LRU eviction policy. It also marks pages dirty which is helpful because it must know if a page needs to be written to disk again when evicting it. Though I don't think there's any reason a page couldn't mark itself dirty when its insertTuple method is called, except for the fact that its method doesn't have a parameter for TransactionId. The bufferpool calls insertTuple in HeapFile, which iterates through pages until it finds one with an open slot. Or if there aren't any open slots it makes a new page. Now that I think about it I imagine it'd be far more efficient to insert everything all at once, like the Insert Operator, or at least keep track of the page of the last insertion so that every insertion doesn't scan the whole file, but I didn't think of that when implementing it. Finally insertTuple on the page itself is called which places a copy of the tuple with updated RecordId into the first open slot it finds and updates its header accordingly.

I used a hash join to speed up equality joins because it's immensely faster and equality joins are common. My implementation of it isn't great because it could crash if there isn't enough memory. It's made worse by the fact that I store all the tuples in a HashMap, which probably greatly exceeds the memory limits any user intended by limiting the size of the bufferpool and makes the memory needed even higher. Though I'm not sure storing a tuple is much worse than storing a RecordId. I could've caught the exception which would be thrown when it's out of memory and fallen back to a nested loop join, though such a query would probably take days to finish with a nested loop join anyway, so I'm not sure it's much better. What would really be better would be a grace hash join, but I just didn't have time.

I chose to make HeapPage.insertTuple copy the tuples it inserts. This prevents changes made to one tuple from updating other tuples. Though I don't think the end user would be able to mutate tuples, at least in our implementation, I did think of one case where this might make a difference: if a query is piping data to a Delete operator. Since tuples are updated with their location upon insertion, inserting the same tuple multiple times would lead some to have the wrong location and multiple Deletes on that tuple would fail because they all would appear to have the same location.

I decided to use a least recently used eviction policy. Initially I considered prioritizing evicting clean pages since they would require no write to disk, but it was more complicated and probably worse than LRU which is simple to implement and effective.

I gave Tuple a bunch of methods that made accessing its various IDs more intuitive (I think). Before I might have had to write someTuple.getRecordId().getPageId().getTableId() but now I can just write someTuple.getTableID(). It was confusing to remember what IDs were accessible and where they were before but it's very clear as all the methods show up while I'm writing IntelliJ. I don't think there are any maintainability downsides to this, as the code for both ways is basically the same so any changes made in the future would affect both similarly. I also added a merge method to it, like the one TupleDesc has, which is similarly convenient for joins.

I think a unit test that inserted multiple tuples and then tried to delete them would be good, since it would catch the mistake of not copying the tuples upon insertion. Also, an issue of mine from last lab actually which I didn't catch is that getNumEmptySlots was reading getHeaderSize()*8 instead of numSlots bits. There's no issue with this when numSlots is a multiple of 8, but if those two numbers differed then it would have been reading junk bits. So a test with tuples of a size which cause this issue to arise would be good.


Some feedback: in the documentation it says that "This tuple should be updated to reflect that it is no longer stored on any page." I thought it would make sense to change the tuple's recordId to null, since it's not stored anywhere anymore but BufferPoolWriteTest doesn't work if I do that. It relies on the tuple's pageId to inspect the page it was just deleted from. So instead all I could do was change the tupleNumber to -1. There's not necessarily an inconsistency here, but I thought some part of that might not have been intentional.